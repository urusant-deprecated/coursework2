\section{Тестирование}

Для тестирования удобно использовать как раз постановку задачи на основе данных. При этом можно использовать как сгенерированные модельные примеры, так и реальные данные. Начнем с первых. Будем генерировать выборку случайно. При этом будем задавать количество признаков и объектов, а также плотность, то есть вероятность появления отдельно заданного признака у заданного объекта. Кроме того, мы можем задавать параметры нашего алгоритма, а именно, $\epsilon, \delta$, а также использовать различные варианты оптимизаций.

Теперь нужно ввести метрики качества. Во-первых, логично измерять качество работы алгоритма, поскольку он не является точным. В качестве метрики качества можно использовать ту величину, для которой мы гарантируем теоретическую оценку. Поскольку объектов экспоненциально много, честный расчет этой величины занимает очень много времени, поэтому будем использовать рандомизированную оценку этой величины: возьмем 10000 случайных объектов, проверим, сколько из них входят в числитель, и усредним размером выборки.

Теперь вернемся к тому, зачем мы вообще хотим обучаться импликациям. В большой степени это необходимо с целью имитации оператора замыкания. Это означает, что для произвольного набора признаков мы хотим понимать, какими еще признаками обладает объект, обладающий этими. Тогда естественной метрикой будет доля объектов, для которых мы правильно определяем замыкание. Ее тоже можно считать рандомизированно: возьмем выборку из 1000 случайных объектов и проверим, для скольких из них замыкание на основе наших импликаций совпадает с замыканием на основе реальных импликаций, и также усредним размером выборки.

Теперь разберемся с параметрами алгоритма. В первую очередь посмотрим на оптимизации. Eсть 3 основных версии алгоритма:

\begin{enumerate}
	\item Базовый алгоритм. Эта версия не использует никаких оптимизаций, кроме, возможно, ограничения по времени.
	\item Улучшенный алгоритм. Эта версия использует оптимизацию 2 из предыдущей секции для задачи на основе данных.
	\item Улучшенный алгоритм с улучшенным оракулом. Эта версия использует обе оптимизации 2 и 3 из предыдущей секции. Отметим, что оптимизация 3 частично основана на гарантиях, предоставляемых оптимизацией 2, и, соответственно, имеет мало смысла без нее.
\end{enumerate}

Кроме того, используем первую оптимизацию следующим образом: возьмем из [2] оптимизированный алгоритм Гантера с использованием линейного замыкания, и ограничим половиной времени его работы наш алгоритм.

Посмотрим на зависимости качества работы алгоритмов от параметров выборки. Ниже приведены графики зависимости 2х метрик, о которых говорилось выше, от одного из параметров выборки при фиксированных двух оставшихся. Здесь guaranteed metric - это метрика, теоретическая оценка на которую гарантируется в оригинальной версии алгоритма, а quality - доля ошибок оператора замыкания.

\includegraphics[scale=0.3]{img/obj-accuracy}
\includegraphics[scale=0.3]{img/obj-quality}

\includegraphics[scale=0.3]{img/attr-accuracy}
\includegraphics[scale=0.3]{img/attr-quality}

\includegraphics[scale=0.3]{img/dens-accuracy}
\includegraphics[scale=0.3]{img/dens-quality}

Как можно видеть, лучшим качеством обладает оригинальный алгоритм. Это, вероятно, происходит потому, что оптимизации хоть и потенциально улучшают качество, значительно ухудшают производительность алгоритма, и поэтому в условиях ограниченного времени не успевают достичь требуемого качества.

Помимо этого, явно прослеживается улучшение качества при росте количества признаков и объектов, то есть, при увеличении размера выборки, что логично. Что же касается плотности выборки, ее увеличение также улучшает качество работы алгоритмов, за исключением значений quality для версии 1, что может объясняться маленьким размером выборки.

Итак, мы теперь понимаем зависимость качества от параметров контекста. Посмотрим теперь на зависимость времени работы от них. Хотя мы и ограничиваем его половиной времени работы точного алгоритма, наш алгоритм может завершиться намного раньше ограничения.

\includegraphics[scale=0.3]{img/obj-time}
\includegraphics[scale=0.3]{img/attr-time}

\includegraphics[scale=0.3]{img/dens-time}

По графикам видно, что вторая версия всегда упирается в ограничение, а остальные два - почти всегда. % TODO Тут хотелось бы сказать "тем не менее, с качеством у них все в порядке", но это не всегда так. Хотя, для 0й версии это так, за исключением случаев совсем маленького количества атрибутов.

Помимо параметров выборки у нас также есть и параметры алгоритма. Ранее мы просто зафиксировали их на константных значениях, равных $\epsilon = \delta = 0.01$. Заметим, что оба параметра ($\epsilon, \delta$) влияют на работу нашего алгоритма схожим образом, а именно, от них зависит количество попыток подобрать контрпример в запросе эквивалентности. При этом это количество зависит от $\frac{1}{\epsilon}$ линейно, а от $\frac{1}{\delta}$ - логарифмически, соответственно, нет смысла смотреть, как влияют изменения разных параметров на работу алгоритма, достаточно посмотреть на один из них. При этом логичнее изменять $\epsilon$, потому что от него количество итераций зависит сильнее. Теоретически, зависимость должна быть следующая: увеличивая $\epsilon$ и, соответственно, уменьшая количество итераций на каждом шагу, мы увеличиваем вероятность не найти контрпример, а значит, завершиться раньше предела.

% Here come the graphs attr-time for eps=0.01 и eps = 0.05